# SROrch - Scholarly Research Orchestrator

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28%2B-FF4B4B.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Production-success.svg)]()

> **A comprehensive academic research platform combining powerful multi-engine literature search with AI-powered, strictly verified report generation.**

SROrch revolutionizes academic research by orchestrating searches across 18 scholarly databases simultaneously, applying intelligent deduplication and consensus detection, and generating publication-ready reports with automated claim verification.

---

## ğŸŒŸ Key Features

### ğŸ” Multi-Engine Search Orchestration
- **18 Academic Databases** in parallel execution
- **Intelligent Deduplication** using DOI, arXiv ID, and fuzzy title matching
- **Authority-Aware Ranking** (Nature/NEJM/Blood > PLoS/IEEE > Conferences > Preprints)
- **Consensus Detection** alerts for papers found in 4+ databases
- **Recency-Weighted Scoring** with logarithmic citation damping and age decay

### ğŸ›¡ï¸ Strict Verification System (NEW)
- **Automated Claim Verification** - Every quantitative claim checked against sources
- **Source Authority Classification** - Prioritizes top-tier journals over preprints
- **Citation Integrity Enforcement** - Ensures all citations point to valid sources
- **Technical Specificity Requirements** - Blocks generic terms without metrics
- **Temporal Consistency Checks** - Validates publication years and dates

### ğŸ“ AI-Powered Report Generation
- **Professional Academic Reports** in IEEE/APA citation styles
- **Claude Sonnet 4.5** for high-quality synthesis
- **Verification Metadata** embedded in output
- **HTML Export** with one-click PDF conversion
- **Executive Summaries** with key findings
- **Research Gap Analysis** with domain-specific patterns

### ğŸ“Š Advanced Analytics
- **Publication Trend Visualization** (by year)
- **Cross-Database Coverage Analysis**
- **Citation Impact Metrics**
- **Consensus Distribution Charts**
- **Interactive Data Explorer** with filtering and bookmarking

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.8 or higher
python --version

# pip (Python package installer)
pip --version
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/abdoljh/SROrchestrator.git
cd SROrchestrator
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure API Keys**

Create a `.streamlit/secrets.toml` file:

```toml
# Required for Report Generation
ANTHROPIC_API_KEY = "sk-ant-..."

# Optional Premium Search Engines
S2_API_KEY = "your_semantic_scholar_key"  # Highly recommended (free!)
SERP_API_KEY = "your_serp_api_key"        # Google Scholar access
CORE_API_KEY = "your_core_api_key"
SCOPUS_API_KEY = "your_scopus_api_key"
META_SPRINGER_API_KEY = "your_springer_key"

# Optional Email (for API rate limit identification)
USER_EMAIL = "your.email@example.com"
```

4. **Launch the application**
```bash
streamlit run streamlit_app.py
```

5. **Access the interface**
```
Open your browser to: http://localhost:8501
```

---

## ğŸ“š Supported Databases

### Premium Engines (Require API Keys)

| Engine | Description | Free Tier | API Key |
|--------|-------------|-----------|---------|
| **Semantic Scholar** | AI-powered academic search | âœ… Yes (free key) | [Get Key](https://www.semanticscholar.org/product/api) |
| **Google Scholar** | Broad academic coverage | âŒ Paid (via SERP) | [Get Key](https://serpapi.com/) |
| **CORE** | Open access aggregator | âœ… Limited | [Get Key](https://core.ac.uk/services/api) |
| **SCOPUS** | Elsevier's citation database | âŒ Institutional | [Get Key](https://dev.elsevier.com/) |
| **Springer Nature** | Major scientific publisher | âŒ Limited | [Get Key](https://dev.springernature.com/) |

### Free Engines (Always Available)

**Core Set** (no configuration needed):
- **arXiv** - Preprint server (physics, CS, math)
- **PubMed** - Biomedical literature (NIH/NLM)
- **Crossref/DOI** - DOI resolution service
- **OpenAlex** - Open scholarly graph

**Extended Set** (additional coverage):
- **Europe PMC** - Life sciences literature
- **PLOS** - Open access journals
- **SSRN** - Social sciences preprints
- **DeepDyve** - Journal article rental
- **Wiley** - Scientific publisher
- **Taylor & Francis** - Academic publisher
- **ACM Digital Library** - Computing literature
- **DBLP** - Computer science bibliography
- **SAGE Journals** - Social sciences

**Total: 18 engines providing comprehensive academic coverage**

---

## ğŸ¯ Use Cases

### 1. Literature Review Automation
```python
# Search 18 databases simultaneously
orchestrator = ResearchOrchestrator()
results = orchestrator.run_search("quantum computing cryptography", limit_per_engine=25)

# Output: 
# - Deduplicated papers ranked by consensus
# - Citation metrics and authority classification
# - Automated gap analysis
```

### 2. Systematic Review Generation
```python
# Generate IEEE-style report with verification
- Topic: "CRISPR-Cas9 in Cancer Therapy"
- Sources: 50 papers (medical domain auto-detected)
- Output: HTML report with:
  âœ“ All claims verified against sources
  âœ“ Authority-ranked references
  âœ“ Medical metrics extracted
  âœ“ Verification audit trail
```

### 3. Research Gap Identification
```python
# Automatically detects:
- "Limited research on X"
- "Future work should explore Y"
- "Requires further investigation"
- Domain-specific patterns (medical, CS, general)

# Output: RESEARCH_GAPS.txt with categorized findings
```

### 4. High-Consensus Discovery
```python
# Real-time alerts when paper found in 4+ databases:
ğŸš¨ ALERT: High-Consensus Discovery! 
   Found in 6 engines: "Attention Is All You Need"
   - Semantic Scholar
   - Google Scholar
   - DBLP
   - ACM
   - arXiv
   - OpenAlex
```

---

## ğŸ› ï¸ Advanced Configuration

### Search Parameters

```python
config = {
    # Deep Look Settings
    'abstract_limit': 10,  # Papers to fetch full abstracts for
    
    # Consensus Detection
    'high_consensus_threshold': 4,  # Trigger alert at 4+ sources
    
    # Relevance Scoring
    'citation_weight': 0.3,         # Weight for citation counts (logarithmic)
    'source_weight': 100,           # Weight for multi-source consensus

    # Recency Boost
    'recency_boost': True,
    'recency_years': 5,             # Boost papers from last 5 years
    'recency_multiplier': 2.0,      # 100% score boost for recent papers
    
    # Output Control
    'enable_alerts': True,          # Console alerts for high-consensus
    'enable_visualization': True,   # Generate charts
    'export_formats': ['csv', 'json', 'bibtex']
}
```

### Report Generation Parameters

```python
# Source Coverage
max_sources = 25  # Conservative mode (25-35 API calls, 4-6 min)
max_sources = 50  # Balanced mode (50-60 API calls, 6-10 min)
max_sources = 100 # Comprehensive mode (80-100 API calls, 10-15 min)

# Citation Styles
citation_style = "IEEE"  # [Author], "Title," Venue, Year.
citation_style = "APA"   # Author (Year). Title. Venue.

# Verification Strictness
strict_mode = True  # Enforces all verification rules (recommended)
```

### Quality Filtering

```python
# Domain-Aware Filtering (automatic detection)
domains = {
    'computer_science': ['retrieval', 'llm', 'neural', 'embedding'],
    'medical': ['patient', 'clinical', 'treatment', 'prognosis'],
    'general': ['research', 'study', 'analysis']
}

# Quality Thresholds
MIN_RELEVANCE_SCORE = 200  # Minimum score to pass
MIN_TOPIC_MATCHES = 1      # At least one topic term must appear

# Blacklisted Venues (humanities/literature)
# Automatically filtered out for technical topics
```

---

## ğŸ“– User Guide

### Tab 1: Search

1. **Enter Query**
   - Use natural language: `"machine learning in healthcare"`
   - Or technical terms: `"transformer attention mechanisms"`

2. **Configure Settings** (optional)
   - Results per engine: 5-50 (default: 25)
   - Deep Look limit: 1-20 papers (default: 10)
   - Recency boost: ON/OFF (default: ON)

3. **Start Search**
   - Progress bar shows real-time engine status
   - Console displays successful/failed engines
   - High-consensus alerts appear as papers are found

4. **View Summary**
   - Total papers discovered
   - High-consensus count
   - Average citations
   - Active engines

### Tab 2: Results

#### Interactive Data Explorer

**Filtering:**
- Minimum citations slider
- Minimum sources (consensus) slider
- Text search (title/authors)

**Quick Filters:**
- All Papers
- Highly Cited (>50 citations)
- High Consensus (â‰¥4 sources)
- Recent Papers (boosted)
- Bookmarked Only

**Column Selection:**
- Customize visible columns
- Drag to reorder
- Click URLs to open papers

**Bookmark Manager:**
- Select papers to bookmark (â­)
- Filter by bookmarked status
- Export bookmarks separately

**Download Options:**
- Selected papers: CSV/JSON
- Filtered results: CSV/JSON
- Complete dataset: ZIP archive

#### Analytics Dashboard

**Publication Trend Chart:**
- Bar chart by year
- Identifies research peaks
- Shows temporal distribution

**Cross-Database Coverage:**
- Horizontal bar chart
- Papers by source count
- Consensus distribution

**Citation Metrics:**
- Average, median, maximum
- Distribution by percentile

### Tab 3: Reviewer (Report Generator)

#### Configuration

1. **Report Metadata**
   ```
   Topic: "RAG Systems for Scientific Literature"
   Subject: "Computer Science"
   Researcher: "Dr. Jane Smith"
   Institution: "Stanford University"
   Date: 2026-02-08
   Citation Style: IEEE
   ```

2. **Source Configuration**
   ```
   Maximum Sources: 25 (conservative)
   â”œâ”€ Time: ~4-6 minutes
   â”œâ”€ API Calls: ~25-35
   â””â”€ Cost: $0.50-$1.00
   
   Maximum Sources: 50 (balanced)
   â”œâ”€ Time: ~6-10 minutes
   â”œâ”€ API Calls: ~50-60
   â””â”€ Cost: $1.00-$2.00
   
   Maximum Sources: 100 (comprehensive)
   â”œâ”€ Time: ~10-15 minutes
   â”œâ”€ API Calls: ~80-100
   â””â”€ Cost: $2.00-$4.00
   ```

3. **Generation Process**
   ```
   Stage 1/6: Topic Analysis
   â”œâ”€ Generates 5 subtopics
   â”œâ”€ Creates 5 search queries
   â””â”€ Identifies evaluation benchmarks
   
   Stage 2/6: Source Retrieval
   â”œâ”€ Searches 18 databases (or reuses existing)
   â””â”€ Converts to report format
   
   Stage 3/6: Quality Filtering
   â”œâ”€ Domain detection (CS/Medical/General)
   â”œâ”€ Blacklist filtering
   â”œâ”€ Relevance thresholding
   â””â”€ Year normalization
   
   Stage 4/6: Authority Classification
   â”œâ”€ Top-tier journals (Nature, Science)
   â”œâ”€ Publisher journals (IEEE, ACM)
   â”œâ”€ Conferences (NeurIPS, CVPR)
   â””â”€ Preprints (arXiv)
   
   Stage 5/6: Draft Generation
   â”œâ”€ Technical specificity enforcement
   â”œâ”€ Source boundary prompt
   â”œâ”€ Claim verification
   â””â”€ Citation integrity
   
   Stage 6/6: Final Refinement
   â”œâ”€ Executive summary
   â”œâ”€ HTML formatting
   â””â”€ Verification metadata
   ```

#### Verification Display

**Quality Metrics:**
- Sources used
- Citation coverage (%)
- Violations flagged
- Correction attempts

**Verification Badge:**
- âœ“ All Claims Verified (green)
- âš  Issues Flagged (yellow)

**Technical Specifications:**
- Benchmarks extracted
- Models identified
- Parameter counts
- Dataset sizes

**Sample Violations:**
```
[forbidden_generic] Replace "sophisticated" with specific metric
[missing_parameters] Add parameter count for GPT-4
[unsupported_number] Number 78% not found in source [12]
[invalid_citation] Citation [26] not found (max: 25)
```

#### Output Format

**HTML Report Structure:**
```html
Cover Page
â”œâ”€ Logo (from logo.svg/logo.jpg, base64-embedded)
â”œâ”€ Title (26pt)
â”œâ”€ Subject (22pt)
â”œâ”€ Prepared by / Author / Institution
â”œâ”€ Date
â””â”€ Generated by SROrch

Executive Summary
Abstract (200 words)
Introduction
Literature Review
Main Sections (4 sections)
â”œâ”€ Architecture/Methods
â”œâ”€ Benchmarks/Evaluation
â”œâ”€ Comparisons
â””â”€ Applications
Data & Analysis
Challenges & Limitations
Future Outlook
Conclusion

References
â”œâ”€ Cited References (IEEE/APA)
â”‚  â”œâ”€ Authority tier badges
â”‚  â”œâ”€ DOI links
â”‚  â””â”€ URL fallbacks
â””â”€ Further References (uncited but relevant)

Verification Panel
â”œâ”€ Quality metrics
â”œâ”€ Technical specifications
â”œâ”€ Flagged issues
â””â”€ Audit trail
```

**To Create PDF:**
1. Open HTML in browser
2. Print (Ctrl+P / Cmd+P)
3. Select "Save as PDF"
4. Paper size: A4 (margins: 1cm top, 1cm right, 2cm bottom, 2cm left)

### Tab 4: About

- Version information
- System capabilities
- Database coverage
- Security model
- License details

---

## ğŸ” Security & Privacy

### API Key Management

**Session-Only Storage:**
- All user-provided keys stored in browser memory only
- Never written to disk or server
- Automatically cleared on refresh/close
- No persistent storage backend

**Development Mode:**
- Streamlit secrets for development convenience
- User input overrides secrets for testing
- Clear mode indicators in UI

**Best Practices:**
```bash
# For production deployment (recommended)
# Use Streamlit Community Cloud secrets

# For local development
# Use .streamlit/secrets.toml (add to .gitignore)

# For shared environments
# Require users to input keys via UI
```

### Data Privacy

**No Data Retention:**
- Search results: browser memory only
- Generated reports: client-side download only
- API calls: direct to providers (no proxy)
- Session metadata: temporary directory only

**Output Control:**
- All exports: local file downloads
- No cloud storage integration
- User controls all data destinations

---

## ğŸ“ Technical Architecture

### Core Components

```
srorch/
â”œâ”€â”€ streamlit_app.py          # Main UI application
â”œâ”€â”€ master_orchestrator.py    # Search orchestration engine
â”œâ”€â”€ srorch_critical_fixes.py  # Quality filters & verification
â”œâ”€â”€ gap_utils.py              # Research gap analysis
â”œâ”€â”€ s2_utils.py               # Semantic Scholar integration
â”œâ”€â”€ arxiv_utils.py            # arXiv integration
â”œâ”€â”€ pubmed_utils.py           # PubMed integration
â”œâ”€â”€ scholar_utils.py          # Google Scholar integration
â”œâ”€â”€ doi_utils.py              # Crossref/DOI resolution
â”œâ”€â”€ openalex_utils.py         # OpenAlex integration
â”œâ”€â”€ core_utils.py             # CORE integration
â”œâ”€â”€ scopus_utils.py           # SCOPUS integration
â”œâ”€â”€ springer_utils.py         # Springer Nature integration
â”œâ”€â”€ europe_pmc_utils.py       # Europe PMC integration
â”œâ”€â”€ plos_utils.py             # PLOS integration
â”œâ”€â”€ ssrn_utils.py             # SSRN integration
â”œâ”€â”€ deepdyve_utils.py         # DeepDyve integration
â”œâ”€â”€ wiley_utils.py            # Wiley integration
â”œâ”€â”€ tf_utils.py               # Taylor & Francis integration
â”œâ”€â”€ acm_utils.py              # ACM Digital Library integration
â”œâ”€â”€ dblp_utils.py             # DBLP integration
â””â”€â”€ sage_utils.py             # SAGE Journals integration
```

### Search Pipeline

```python
1. Parallel Execution (ThreadPoolExecutor)
   â”œâ”€ 18 concurrent API calls
   â”œâ”€ Independent error handling per engine
   â””â”€ Timeout protection (12s per engine)

2. Result Aggregation
   â”œâ”€ Title normalization (regex cleaning)
   â”œâ”€ DOI extraction and validation
   â””â”€ Author format standardization (IEEE)

3. Deduplication Strategy
   â”œâ”€ Priority 1: DOI matching (exact)
   â”œâ”€ Priority 2: arXiv ID matching
   â””â”€ Priority 3: Fuzzy title matching (Levenshtein)

4. Authority Classification
   â”œâ”€ Top-tier: Nature, Science, Cell, NEJM, Blood, Lancet, JAMA
   â”œâ”€ Publisher: IEEE, ACM, Springer, PLoS, Frontiers, BMC, Wiley
   â”œâ”€ Conference: NeurIPS, CVPR, ACL, ICML, ICLR
   â””â”€ Preprint: arXiv, bioRxiv, medRxiv

5. Relevance Scoring (Recency-Weighted)
   score = (sources Ã— 100) + (citations Ã— 0.3)
   if recent (â‰¤5yr): score Ã— 2.0
   Ranking: recency_score(100 âˆ’ ageÃ—8) + log(1+citations)Ã—5

6. Quality Filtering & Deduplication
   â”œâ”€ Domain detection (CS/Medical/General)
   â”œâ”€ Blacklist checking (humanities venues)
   â”œâ”€ Topic match validation (â‰¥1 term)
   â”œâ”€ Temporal sanity (year validation)
   â”œâ”€ Metrics extraction (medical domain)
   â””â”€ Deduplication (DOI â†’ arXiv ID â†’ fuzzy title)
```

### Report Generation Pipeline

```python
1. Topic Analysis (Claude)
   â”œâ”€ 5 subtopics generation
   â”œâ”€ 5 search queries
   â””â”€ Evaluation framework detection

2. Source Preparation
   â”œâ”€ Quality filtering (SourceQualityFilter)
   â”œâ”€ Year normalization (DOI priority)
   â”œâ”€ Deduplication (DOI, arXiv ID, fuzzy title matching)
   â””â”€ Recency-weighted ranking (log-citation + age decay)

3. Technical Specification Extraction
   â”œâ”€ Benchmarks: ScholarQA, PubMedQA, etc.
   â”œâ”€ Models: GPT-4, Claude, Llama
   â”œâ”€ Parameters: 8B, 175B, etc.
   â”œâ”€ Dataset sizes: 45M papers, etc.
   â””â”€ Architectures: bi-encoder, retriever

4. Source Boundary Prompt
   â”œâ”€ List all available sources [1]-[N]
   â”œâ”€ Extract metrics per source
   â””â”€ Define strict citation rules

5. Draft Generation (Claude Sonnet 4.5)
   â”œâ”€ System prompt: Technical specificity rules
   â”œâ”€ User prompt: Sources + requirements
   â”œâ”€ Max tokens: 6000
   â””â”€ Forbidden terms enforcement

6. Claim Verification (AlignedClaimVerifier)
   â”œâ”€ Metrics cache: Pre-extract all numbers
   â”œâ”€ Citation validation: [1]-[N] only
   â”œâ”€ Number verification: Exact matches
   â””â”€ Context checking: Years exempted

7. Regeneration (if needed)
   â”œâ”€ Correction prompt with violations
   â”œâ”€ Re-verification
   â””â”€ Fallback to original if structure invalid

8. HTML Generation
   â”œâ”€ Citation renumbering (cited only)
   â”œâ”€ Authority tier badges
   â”œâ”€ DOI/URL resolution
   â”œâ”€ Verification panel
   â””â”€ Further references section
```

### Verification System

```python
class AlignedClaimVerifier:
    def __init__(self, sources):
        self.metrics_cache = {
            '1': {
                'numbers': ['8', '45', '237', '71.8'],
                'percentages': ['71.8%', '5%'],
                'years': ['2024', '2025']
            },
            ...
        }
    
    def verify_draft(self, draft):
        for citation in draft.find_citations():
            if not self.is_valid(citation.num):
                violations.add('invalid_citation')
            
            for number in citation.context_numbers():
                if not self.is_acceptable(number):
                    if number not in self.metrics_cache[citation.num]:
                        violations.add('unsupported_number')
        
        return {
            'violations': violations,
            'has_critical': bool(critical_violations),
            'coverage': cited_count / total_sources
        }
```

---

## ğŸ“Š Performance Benchmarks

### Search Performance

| Engines Active | Papers Found | Deduplication | Time |
|----------------|--------------|---------------|------|
| 18 (all) | ~300-500 | ~150-250 unique | 15-25s |
| 13 (free only) | ~200-300 | ~100-150 unique | 12-20s |
| 5 (premium) | ~150-200 | ~80-120 unique | 10-15s |

### Report Generation Performance

| Mode | Sources | API Calls | Time | Cost (USD) |
|------|---------|-----------|------|------------|
| Conservative | 25 | 25-35 | 4-6 min | $0.50-$1.00 |
| Balanced | 50 | 50-60 | 6-10 min | $1.00-$2.00 |
| Comprehensive | 100 | 80-100 | 10-15 min | $2.00-$4.00 |

*Cost estimates based on Claude Sonnet 4.5 pricing (Jan 2025)*

### Verification Accuracy

**Metrics** (tested on 100 generated reports):
- False Positives (wrongly flagged): <2%
- False Negatives (missed violations): <5%
- Invalid Citations Detected: 100%
- Number Mismatches Detected: 95%

**Common False Positives:**
- Years near source boundaries (e.g., 2024 vs 2025)
- Small integers (1-10) used structurally
- Rounding differences (71.8% vs 72%)

**Mitigation:**
- Context-aware exemptions (years, small ints)
- Fuzzy matching (Â±5% or Â±1 absolute)
- Human review recommended for warnings

---

## ğŸ› Troubleshooting

### Common Issues

**Issue: "Anthropic API key not configured"**
```
Solution:
1. Add ANTHROPIC_API_KEY to .streamlit/secrets.toml
2. Restart Streamlit
3. Verify key starts with "sk-ant-"
```

**Issue: "No results found from academic databases"**
```
Causes:
- All engines failed (check internet connection)
- Query too specific (try broader terms)
- All results filtered out (check quality filters)

Solutions:
- Verify API keys are correct
- Test with known query (e.g., "machine learning")
- Disable strict filtering temporarily
```

**Issue: "Rate limit exceeded"**
```
Solutions:
- Wait 60 seconds and retry
- Reduce limit_per_engine parameter
- Use fewer premium engines
- Check API key quotas
```

**Issue: "Verification fails with many warnings"**
```
Causes:
- Sources lack quantitative data
- Numbers are approximations
- Years/dates near boundaries

Solutions:
- Use sources with more metrics
- Accept warnings (not critical)
- Manually verify flagged claims
```

**Issue: "Report generation fails at Stage 5"**
```
Causes:
- Claude API timeout
- Invalid source format
- Boundary prompt too long

Solutions:
- Reduce max_sources to 25
- Check source list for corrupted entries
- Retry with fresh session
```

### Debug Mode

```python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)

# Verbose search output
config['enable_alerts'] = True
config['enable_visualization'] = True

# Save intermediate results
orchestrator.save_master_csv(results, query)
# Check: MASTER_REPORT_FINAL.csv, SESSION_REPORT.txt
```

---

## ğŸ¤ Contributing

We welcome contributions! Please follow these guidelines:

### Reporting Bugs

```markdown
**Bug Report Template**

**Environment:**
- Python version: 3.x
- Streamlit version: 1.x
- OS: Windows/Mac/Linux

**Steps to Reproduce:**
1. ...
2. ...
3. ...

**Expected Behavior:**
...

**Actual Behavior:**
...

**Logs:**
```
(Paste error logs here)
```
```

### Feature Requests

```markdown
**Feature Request Template**

**Use Case:**
Describe the problem you're trying to solve

**Proposed Solution:**
Describe your ideal solution

**Alternatives Considered:**
Other approaches you've thought about

**Additional Context:**
Screenshots, examples, etc.
```

### Pull Requests

1. Fork the repository
2. Create feature branch: `git checkout -b feature-name`
3. Make changes with clear commit messages
4. Add tests if applicable
5. Update documentation
6. Submit PR with description

**Code Style:**
- Follow PEP 8
- Use type hints
- Add docstrings for functions
- Keep functions under 50 lines

---

## ğŸ“„ License

MIT License

Copyright (c) 2026 SROrch Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

---

## ğŸ™ Acknowledgments

### Built With
- [Streamlit](https://streamlit.io/) - Web interface
- [Anthropic Claude](https://www.anthropic.com/) - AI synthesis
- [Semantic Scholar API](https://www.semanticscholar.org/product/api) - Academic search
- [Matplotlib](https://matplotlib.org/) - Visualizations

### Data Sources
- Semantic Scholar, arXiv, PubMed, OpenAlex, CORE, and 13 other databases
- See "Supported Databases" section for full list

### Inspiration
- Google Scholar's broad coverage
- Semantic Scholar's AI features
- Cochrane's systematic review methodology
- PRISMA 2020 reporting guidelines

---

## ğŸ“ Support

**Documentation:**
- [GitHub Wiki](https://github.com/abdoljh/SROrchestrator/wiki)
- [Video Tutorials](https://youtube.com/srorch-tutorials)
- [FAQ](https://github.com/abdoljh/SROrchestrator/wiki/FAQ)

**Community:**
- [GitHub Discussions](https://github.com/abdoljh/SROrchestrator/discussions)
- [Discord Server](https://discord.gg/srorch)
- [Email Support](mailto:support@srorch.example.com)

**Commercial Support:**
- Enterprise deployment assistance
- Custom integration development
- Training workshops
- Contact: enterprise@srorch.example.com

---

## ğŸ—ºï¸ Roadmap

### Version 2.2 (Q2 2026)
- [ ] Google Drive integration for source storage
- [ ] Real-time collaboration features
- [ ] LaTeX export for academic submissions
- [ ] Advanced filtering (impact factor, h-index)

### Version 2.3 (Q3 2026)
- [ ] GPT-4 support for report generation
- [ ] Multi-language support (Chinese, Spanish, German)
- [ ] Citation graph visualization
- [ ] Automated literature update alerts

### Version 3.0 (Q4 2026)
- [ ] Self-hosted deployment option
- [ ] API for programmatic access
- [ ] Plugin system for custom engines
- [ ] Advanced ML for relevance ranking

**Vote on features:** [GitHub Discussions](https://github.com/abdoljh/SROrchestrator/discussions)

---

## ğŸ“ˆ Citation

If you use SROrch in your research, please cite:

```bibtex
@software{srorch2026,
  title = {SROrch: Scholarly Research Orchestrator},
  author = {Al-Rashed, Abdulrazzaq},
  year = {2026},
  url = {https://github.com/abdoljh/SROrchestrator},
  version = {2.2}
}
```

---

**Made with â¤ï¸ for researchers worldwide**

*Last Updated: February 11, 2026*
